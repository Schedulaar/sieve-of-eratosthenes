
\documentclass[a4paper,12pt]{article} 
\usepackage[top = 2.5cm, bottom = 2.5cm, left = 2.5cm, right = 2.5cm]{geometry} 

\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage[utf8]{inputenc}
\usepackage{algorithm,algpseudocode}
\usepackage{float}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{xcolor}
\newtheorem{proposition}{Proposition}
\newtheorem{theorem}{Theorem}
\bibliographystyle{plain}

\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{multirow} 
\usepackage{booktabs}
\usepackage{graphicx} 
\usepackage{listings}
\lstset{language=C++,
	numbers=none,
	showspaces=false,
	showstringspaces=false,
	basicstyle=\footnotesize}

\usepackage{setspace}
\setlength{\parindent}{0in}

\usepackage{float}

\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}

\usepackage{mathtools}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}
\newcommand{\bigO}{\mathcal{O}}
\lhead{\footnotesize The Sieve of Eratosthenes}
\rhead{\footnotesize Michael Markl} 


\cfoot{\footnotesize \thepage} 


\newcommand{\todo}[1]{{\color{red}#1}}

\DeclareMathOperator{\currPrime}{currPrime}
\DeclareMathOperator{\currCoord}{currCoord}
\DeclareMathOperator{\lastCoord}{lastCoord}
\DeclareMathOperator{\crosses}{crosses}
\renewcommand{\lstlistingname}{Program}
\newcommand{\algorithmautorefname}{Algorithm}
\newcommand{\propositionautorefname}{Proposition}


\begin{document}

\thispagestyle{empty}

\begin{tabular}{p{15.5cm}} %
{\large \bf Parallel Algorithms} \\
University of Utrecht \\ Fall 2020  \\ Rob H. Bisseling\\
\hline
\\
\end{tabular}

\vspace*{0.3cm}
\begin{center}
	{\Large \bf The Sieve of Eratosthenes}
	\vspace{2mm}
	
	{\bf Michael Markl}
\end{center}  
\vspace{0.4cm}

\section{An ancient primality test}

Prime numbers play a central role for example in modern encryption algorithms.
Most encryption algorithms need prime numbers with about 1,000 digits as a secret key.
Hence, many algorithms chose some number randomly and then check with so-called primality tests, whether this number is a prime number and therefore suitable for using as an encryption key.

One such primality test, which was already invented by the ancient greeks, is called the Sieve of Eratosthenes.
This algorithm not only checks for a given number whether it is prime, but it even finds all prime numbers up to that number.
Given the number $n$ the procedure starts with a list of the numbers from $2$ to $n$.
It starts with $2$ and cross out all larger multiples of $2$.
Then it continues with the next number which is not crossed out -- in the second iteration the number $3$ -- and crosses out all larger multiples of it.
This procedure is repeated until the whole list is worked through.
Then all numbers that are not crossed out in the end are exactly all prime numbers up to the number $n$.

For generating all prime numbers up to $n$ it even suffices to iterate this procedure until the number~$\floor{\sqrt{n}}$: If we assume, that $x\leq n$ is not a prime number, we find numbers $a,b\in\{2, \dots, x\}$ with $x=a\cdot b$.
If $a$ or $b$ is smaller than or equal to $\floor{\sqrt{n}}$ then $x$ would be crossed out after reaching $\floor{\sqrt{n}}$.
Otherwise, $a$ and $b$ are strictly larger than $\floor{\sqrt{n}}$ which implies that $a$ and $b$ are strictly larger than $\sqrt{n}$ and hence $x = a\cdot b > \sqrt{n}^2 = n \geq x$ forms a contradiction.

Even faster: Once we choose to erase all larger multiples of some number, we can choose to start with its square, as all other multiples -- if existent -- have already been taken care of when crossing out multiples of smaller numbers.

This very simple algorithm is implemented in~\autoref{prg:seq_sieve} using the programming language C++.
It initalises an array of size $n-1$ for the numbers $2,\dots, n$ with the value $\mathbf{false}$.
We now interpret $\crosses[i] == \mathbf{true}$ as the number $i+2$ being crossed out, and sequentially go through all numbers up to $n$ and cross out larger multiples of primes as explained above.
Operations involving the variable $\mathrm{numberOps}$ in~\autoref{prg:seq_sieve} are only used for numerically evaluating the cost of the algorithm, which we will come to after analysing the asymptotic cost.

\begin{proposition}\label{prop:seq_sieve}
	The sequential implementation in~\autoref{prg:seq_sieve} has cost $\bigO(n \log \log n)$.
\end{proposition}
\begin{proof}
	We may assume that computing the square root, and that the allocation of memory has constant cost.
	Then the initialisation of the variables takes $\bigO(n)$ steps.
	The outer loop has at most $\floor{\sqrt{n}}$ iterations and for each iteration, if a prime number $p$ was found, it takes another $\floor{n / p}$ to cross out all larger multiples of $p$.
	Using that the probability that a natural number $x$ is a prime number is about $1/\ln x$, we can bound the overall asymptotic expected cost of the outer loop by
	\[
		\sqrt n + \sum_{\substack{p = 2 \\ \text{$p$ is prime}}}^{\floor{\sqrt{n}}} \frac{n}{p} \leq \sqrt{n} + n \sum_{k = 2}^n \frac{1}{k \ln k}
	\]
	Approximating $\sum_{k=2}^n 1/(k\ln k)$ by the integral $\int_2^n 1 / (x \ln x) \,\mathrm{d}x$ yields the asymptotic bound
	\[
		n \int_2^n \frac{1}{x \ln x}\, \mathrm{d}x = n \left[ \ln(\ln(x)) \right]_2^n = \bigO(n\log\log n ).
	\]
	As the second only iterates through all numbers and collects those numbers that are not crossed out, this proves the proposition.
\end{proof}


\begin{figure}
	\caption{Numerical analysis of the cost of the sequential Sieve of Eratosthenes}
	\centering
	\label{fig:seq_sieve}
	\begin{tikzpicture}
	\begin{loglogaxis}[
	scale only axis,
	width=0.8\textwidth,
	xmin=1,
	ymin=1,
	log basis x=2,
	log basis y=2,
	height=4cm,
	xlabel=$n$, 
	ylabel=$\mathrm{numberOps}$,
	grid=major]
	\addplot[only marks, mark=+] coordinates {
		(2,7)
		(4,23)
		(8,49)
		(16,116)
		(32,244)
		(64,521)
		(128,1078)
		(256,2243)
		(512,4639)
		(1024,9560)
		(2048,19627)
		(4096,40259)
		(8192,82429)
		(16384,168497)
		(32768,343690)
		(65536,700295)
		(131072,1425109)
		(262144,2896784)
		(524288,5882008)
		(1048576,11932592)
		(2097152,24188194)
		(4194304,48993641)
		(8388608,99164080)
		(16777216,200581343)
		(33554432,405485584)
		(67108864,819286034)
		(134217728,1654567969)
		(268435456,3339904925)
		(536870912,6739134949)
		(1073741824,13592834417)
	};
	\end{loglogaxis}
	\end{tikzpicture}
	
	\begin{tikzpicture}
	\begin{semilogxaxis}[
	scale only axis,
	width=0.8\textwidth,
	xmin=1,
	log basis x=2,
	log basis y=2,
	height=4cm,
	xlabel=$n$, 
	ylabel=$\Delta(\mathrm{numberOps})$,
	grid=major]
	\addplot[] coordinates {
		(2.001000,5.750000)
		(4,5.750000)
		(4.001000,6.125000)
		(8,6.125000)
		(8.001000,7.250000)
		(16,7.250000)
		(16.001000,7.625000)
		(32,7.625000)
		(32.001000,8.140625)
		(64,8.140625)
		(64.001000,8.421875)
		(128,8.421875)
		(128.001000,8.761719)
		(256,8.761719)
		(256.001000,9.060547)
		(512,9.060547)
		(512.001000,9.335938)
		(1024,9.335938)
		(1024.001000,9.583496)
		(2048,9.583496)
		(2048.001000,9.828857)
		(4096,9.828857)
		(4096.001000,10.062134)
		(8192,10.062134)
		(8192.001000,10.284241)
		(16384,10.284241)
		(16384.001000,10.488586)
		(32768,10.488586)
		(32768.001000,10.685654)
		(65536,10.685654)
		(65536.001000,10.872719)
		(131072,10.872719)
		(131072.001000,11.050354)
		(262144,11.050354)
		(262144.001000,11.219040)
		(524288,11.219040)
		(524288.001000,11.379807)
		(1048576,11.379807)
		(1048576.001000,11.533830)
		(2097152,11.533830)
		(2097152.001000,11.680994)
		(4194304,11.680994)
		(4194304.001000,11.821280)
		(8388608,11.821280)
		(8388608.001000,11.955580)
		(16777216,11.955580)
		(16777216.001000,12.084412)
		(33554432,12.084412)
		(33554432.001000,12.208313)
		(67108864,12.208313)
		(67108864.001000,12.327492)
		(134217728,12.327492)
		(134217728.001000,12.442115)
		(268435456,12.442115)
		(268435456.001000,12.552617)
		(536870912,12.552617)
		(536870912.001000,12.659314)
		(1073741824,12.659314)
	};
	\end{semilogxaxis}
	\end{tikzpicture}
	
\end{figure}


Now, for the numerical analysis we may assume that summations, mulitplications and taking square roots as well as evaluating comparisons takes one operation.
Then the variable $\mathrm{numberOps}$ counts the number of operations used in~\autoref{prg:seq_sieve}.
The first graph in~\autoref{fig:seq_sieve} shows the number of operations used for a given number $n$.
As expected, the factor of $\log\log n$ does not play any role at first glance as the plot given by the simulation strongly resembles a linear plot.
But having a closer look at the numbers, we can actually find out, that the number of operations is not linear:
Using the difference quotient for intervals $I\coloneqq [2^k, 2^{k+1}]$ defined as
\[\Delta(\mathrm{numberOps})_I\coloneqq \frac{\mathrm{numberOps}(2^{k+1}) - \mathrm{numberOps}(2^{k})}{2^k},\]
we can obtain more fine-grain statements about the growth-rate of the number of operations.
In the second graph of~\autoref{fig:seq_sieve} this series of difference quotients is displayed which shows that the growth is increasing and hence the number of operations is not linear just like in~\autoref{prop:seq_sieve}.
In fact, it supports the statement that the bound in~\autoref{prop:seq_sieve} is asymptotically tight, as the graph resembles the logarithm where the $x$-axis itself is logarithmic.
Hence $\Delta(\mathrm{numberOps})$ resembles $\log\log n$.


\section{A parallel algorithm of the Sieve of Eratosthenes}

\subsection{The Algorithm}

When coming up with an idea to parallelise this algorithm, one needs to consider several questions:
What kind of work can be easily parallelised?
How should the numbers be distributed over the processors and what are the implications for designing the parallel algorithm?

First, let us discuss which parts of the algorithm can be worked on simultaneously.
The algorithm roughly consists of two repeated steps:
\begin{enumerate}
	\item Search for the next prime, i.e. the next number that is not crossed out.
	\item Cross out all larger multiples of the selected number.
\end{enumerate}
Trying to parallelise the first step would lead to multiple processors looking for a reasonable number they can use to erase all larger multiples of.
In doing so, it is inevitable that processors might choose some number that turns out not to be prime.
This does not lead to wrong calculations if no number is skipped, because crossing out all larger multiples of a composite number will of course not cross out any prime, but it introduces duplicate work:
If a prime dividing the composite number is chosen by a processor, then all the multiples of the composite number will be checked again.

Hence, the better idea may be to parallelise the second step.
In fact, once a number has been chosen and is known to every processor, each processor can cross out all larger multiples of that number located in its share of all numbers up to $n$.
The next arising question is: Who is responsible for deciding on the multiples of which number to work on next?
It should be a processor which is able to tell the next number that is not crossed out.
Therefore, it makes sense to arrange the numbers in blocks of equal size and start with the first processor to be the so-called \emph{coordinator}, as we know for sure, that for the next couple of iterations the next number, which is not crossed out, will be contained in its block.
Once the processor reaches the end of its block it can transfer the responsibility of coordination on to the next processor.

A benefit of this approach is, that communication between processors is very limited:
In each iteration, the coordinator only has to broadcast either the selected number or the new coordinator.

As we only have to process numbers up to $\sqrt{n}$, a change in coordination responsibility will only be necessary, if there are many processors compared to the number $n$: Each block has $\ceil{n/p}$ numbers, therefore a change of coordinator is not necessary if $p \leq \sqrt{n}$, which is satisfied for high numbers $n$.
Nevertheless, we will take care of this in the algorithm.


\renewcommand{\algorithmicrequire}{\textit{input:}\ \  }
\renewcommand{\algorithmicensure}{\textit{output:}}
\algdef{SE}[DOWHILE]{Do}{doWhile}[1]{\algorithmicdo\ #1}[1]{\algorithmicdo \algorithmicwhile\ #1}%
\algnewcommand{\IIf}[1]{\State\algorithmicif\ #1\ \algorithmicthen}
\algnewcommand{\EndIIf}{\unskip\ \algorithmicend\ \algorithmicif}

\begin{algorithm}
	\caption{Parallel Sieve of Eratosthenes}
	\label{alg:parallel_sieve}
	\begin{algorithmic}[]
		\Require $ n :$ positive number.
		\Ensure $l_1, \dots, l_p :$ list of numbers in each processor, such that their concatenation yields a list of all prime numbers smaller or equal to $n$.
		\State
		\State $\currPrime := 2;$ \Comment{Superstep ($0$)}
		\State $\currCoord := 0;$
		\State $\lastCoord := 0;$
		\State {$\crosses := $ list distributed in blocks $\mathbf{b}_s$ with value $\mathbf{false}$ for indices $2,\dots,n;$}
		\State\While {$\currPrime \leq \sqrt{n}$}
		\State {\{ Sieve with $\currPrime$, if coordinator has not changed \}}\Comment{Superstep ($1_k$)}
		\If {$\currCoord = \lastCoord$}
		\For {$i\in \mathbf{b}_s$ with $i \geq \currPrime^2$ and $i ~\%\,\currPrime = 0$}
		\State $\crosses[i] := \mathbf{true};$
		\EndFor
		\EndIf
		
		\State{\{ Search for next prime or transfer coordination \} }
		\State {$\lastCoord := \currCoord;$}
		\If {$s = \currCoord$}
		\State {\{ Search for next prime \}}
		\Do {$\currPrime++;$}
		\doWhile {$\currPrime \in \mathbf{b}_s$ and $\crosses[\currPrime] = \mathbf{true}$}
		\IIf {$\currPrime \notin \mathbf{b}_s$}{
			$\currCoord++$;
		} \EndIIf
		\EndIf
		
		\State
		\State{\{ Broadcast new information \}}\Comment{Superstep ($2_k$)}
		\If {$s = \lastCoord$}
		\For {$j := 0 ~ \mathbf{to}~ p-1$}
		\State {Put $\currPrime$ to $P(j);$}
		\State {Put $\currCoord$ to $P(j);$}
		\EndFor
		\EndIf
		\EndWhile
		\State
		\State{\{ Accumulate primes in lists \}}\Comment{Superstep ($3$)}
		\State {$l_s := $ empty list}
		\For {$i\in \mathbf{b}_s$ with $\crosses[i] = \mathbf{false}$}
		\State {Append $i$ to $l_s;$}
		\EndFor
	\end{algorithmic}
\end{algorithm}


Bringing all this knowledge into shape yields~\autoref{alg:parallel_sieve}.
In a first step, every processor initialises variables holding the current prime number, the processor id of the current coordinator and an array of boolean values for indices from 2 up to $n$ distributed in blocks.
As in~\autoref{prg:seq_sieve} this list saves for a specific number whether it has been crossed out already.

Now we iterate as long as the current prime is greater than $\sqrt{n}$ and undertake the following two supersteps:
In Superstep ($1_k$) each processor crosses out all larger multiples of the current prime, if the coordinator has not changed since the last iteration.
This is necessary, to be sure that the variable for currentPrime actually holds a prime number and to avoid unnecessary work.
In turn, the coordinator searches for the next prime.
If his search is not successful, it means, that the next prime is to be expected in the block of the next processor.
In that case, he transfers the responsibility for the coordination to the next processor.
In superstep ($2_k$) the coordinator broadcasts this newly gained information to all other processors.
 
Once all numbers up to $\sqrt{n}$ have been processed, we know that the list holding all crosses is finished.
Therefore, we conclude the algorithm with superstep ($3$) by collecting all such numbers of the local block that have not been crossed out and are therefore prime.

\subsection{Cost Analysis}

Let us now estimate the BSP cost of this algorithm:
For that, $l$ denotes the latency for synchronisation and $g$ denotes the gap between sending successive data words.
Superstep ($0$) costs about $n/p$ for initialising the $\crosses$ array.
The sieving loop of Superstep ($1_k$) is executed once for every prime in $2,\dots,\floor{\sqrt{n}}$.
Similarly to the proof of~\autoref{prop:seq_sieve} we get a cost bound of $4\cdot n/p \cdot \ln(\ln(n))$, where we bound the comparisons and index calculations per loop iteration by 4.
As changing the coordinator does practically not occur for large $n$, we ignore the second part of superstep ($1_k$).
Superstep ($2_k$) now involves a communication in a $2(p-1)$-relation.
As the number of primes smaller than or equal to $n$ is approximately $n/\ln n$, this introduces a cost smaller than $2p\cdot n/\ln n\cdot g$.
Also total number of occurences of supersteps ($1_k$) and ($2_k$) is approximately  $n/\ln n$ introducing a communication cost of $n/\ln n \cdot l$.
The last superstep accumulates all primes in a list which takes every processor only $n/p$ operations.
The synchronisation of supersteps ($0$) and ($3$) can be neglected.
Hence the overall estimate of the BSP cost accumulates to
\[
	\frac{2n + 4n \ln \ln n}{p} + \frac{2p n}{\ln n}g + \frac{n}{\ln n} l.
\]

One might consider optimising the algorithm for a speed improvement:
If we let each processor compute the primes up to $\sqrt{n}$ themselves, we could abandon all communication altogether, but this comes with the expense of an additional memory requirement, hence this improvement was not applied here.

For putting this algorithm into an actual parallel program most efforts went into getting the local indices of the $\crosses$ array right.
The parallel program is implemented in two different ways:
In the first attempt, the program was implemented exactly like the algorithm suggests.
As an optimisation, we save half of the space for the $\crosses$ array by ignoring all even numbers, because we know, that $2$ is the only even prime number.
Although ignoring even numbers makes local index computation slightly harder, it does not only save on memory, but as we will see also improves the running time.
The resulting optimised program can be seen in~\autoref{prg:par_opt_sieve}.

\begin{minipage}{\linewidth}	
\begin{figure}[H]
	\caption{Running time analysis of the parallel Sieve of Eratosthenes}
	\label{fig:par_opt_sieve}
	\centering
	\begin{tikzpicture}
	\begin{loglogaxis}[
		scale only axis,
		width=0.8\textwidth,
		log basis x=10,
		log basis y=10,
		ymin=0.000002,
		ymax=200,
		height=6.2cm,
		xlabel=$n$, 
		ylabel=Mean Running Time in $s$,
		title={Unoptimised Program},
		grid=major,
		legend entries={$p=1$,$p=2$,$p=4$,$p=8$,$p=16$},
		legend pos=south east]
		\addplot coordinates {
			(100, 0.0000060000000000000035)
			(1000, 0.000008019999999999986)
			(10000, 0.00005325000000000004)
			(100000, 0.0005608699999999997)
			(1000000, 0.009576549999999998)
			(10000000, 0.16271457)
			(100000000, 6.204124599999999)
			(1000000000, 77.03984354000002)
		};
		
		\addplot coordinates {
			(100, 0.00003313999999999998)
			(1000, 0.000043469999999999995)
			(10000, 0.00011114999999999992)
			(100000, 0.00069373)
			(1000000, 0.005331030000000001)
			(10000000, 0.07657337)
			(100000000, 3.3534415700000006)
			(1000000000, 41.51521570999999)
		};
		\addplot coordinates {
			(100, 0.000052720000000000003)
			(1000, 0.00008104)
			(10000, 0.00018946000000000007)
			(100000, 0.0006461200000000001)
			(1000000, 0.00350927)
			(10000000, 0.03839258)
			(100000000, 2.02312066)
			(1000000000, 21.126782429999995)
		};
		\addplot coordinates {
			(100, 0.00017082000000000002)
			(1000, 0.00026198999999999987)
			(10000, 0.00032855000000000004)
			(100000, 0.00079255)
			(1000000, 0.0033914300000000004)
			(10000000, 0.022402059999999988)
			(100000000, 1.11546878)
			(1000000000, 11.059131900000004)
		};
		
		\addplot coordinates {
			(100, 0.0003873299999999999)
			(1000, 0.0008949600000000001)
			(10000, 0.0011080400000000002)
			(100000, 0.002068410000000001)
			(1000000, 0.00509183)
			(10000000, 0.022208490000000004)
			(100000000, 0.6635081599999999)
			(1000000000, 6.578784989999999)
		};			
		\end{loglogaxis}
	\end{tikzpicture}
	\begin{tikzpicture}
	\begin{loglogaxis}[
	scale only axis,
	width=0.8\textwidth,
	log basis x=10,
	log basis y=10,
	ymin=0.000002,
	ymax=200,
	height=6.2cm,
	xlabel=$n$, 
	ylabel=Mean Running Time in $s$,
	title={Optimised Program},
	grid=major,
	legend entries={$p=1$,$p=2$,$p=4$,$p=8$,$p=16$},
	legend pos=south east]
	\addplot coordinates {
		(100, 0.0000066800000000000055)
		(1000, 0.000010560000000000021)
		(10000, 0.0000605200000000001)
		(100000, 0.0005920100000000001)
		(1000000, 0.00758344)
		(10000000, 0.10046166)
		(100000000, 3.01366798)
		(1000000000, 40.483489250000005)
	};
	\addplot coordinates {
		(100, 0.00008104999999999999)
		(1000, 0.000048960000000000026)
		(10000, 0.00011162999999999999)
		(100000, 0.00049619)
		(1000000, 0.004240320000000002)
		(10000000, 0.049259389999999986)
		(100000000, 1.6486180099999992)
		(1000000000, 21.695564050000016)
	};
	\addplot coordinates {
		(100, 0.000047699999999999994)
		(1000, 0.00009109000000000002)
		(10000, 0.00019095000000000003)
		(100000, 0.0007305999999999999)
		(1000000, 0.003066719999999999)
		(10000000, 0.026568040000000015)
		(100000000, 0.9389143300000005)
		(1000000000, 11.186349019999996)
	};
	\addplot coordinates {
		(100, 0.00045850999999999943)
		(1000, 0.00022281000000000002)
		(10000, 0.0005693099999999997)
		(100000, 0.0008197699999999999)
		(1000000, 0.00289696)
		(10000000, 0.01664076000000001)
		(100000000, 0.4329301399999999)
		(1000000000, 5.802923569999998)
	};
	\addplot coordinates {
		(100, 0.0014837600000000013)
		(1000, 0.0006169300000000001)
		(10000, 0.0008313899999999999)
		(100000, 0.0023080899999999988)
		(1000000, 0.006231130000000005)
		(10000000, 0.01802138)
		(100000000, 0.21772650000000004)
		(1000000000, 4.323914559999998)
	};
	\end{loglogaxis}
	\end{tikzpicture}
\end{figure}


\begin{figure}[H]
	\caption{Relative Standard Deviation of time analysis for~\autoref{fig:par_opt_sieve}}
	\label{fig:par_sieve_rel_deviation}
	\centering
		\begin{tikzpicture}
		\begin{semilogxaxis}[
		scale only axis,
		width=0.4\textwidth,
		log basis x=10,
		log basis y=10,
		height=4cm,
		xlabel=$n$, 
		ylabel=Rel. Deviation,
		ylabel near ticks,
		ymax=7,
		ymin=-0.7,
		title={Unptimised Program},
		grid=major,
		legend entries={$p=1$,$p=2$,$p=4$,$p=8$,$p=16$},
		legend pos=north east]
		\addplot coordinates {
		(100, 0.40340859692355235)
		(1000, 0.02493765586034907)
		(10000, 0.05088096164174562)
		(100000, 0.0038067790454632154)
		(1000000, 0.020418533651413596)
		(10000000, 0.25293744528433115)
		(100000000, 0.025694683169385966)
		(1000000000, 0.01598727793789068)
		
		};
		
		\addplot coordinates {
		(100, 0.1619031789957998)
		(1000, 0.05195169822932531)
		(10000, 0.03367224114302749)
		(100000, 0.13299049994172962)
		(1000000, 0.007874813131699599)
		(10000000, 0.08365855830144216)
		(100000000, 0.026867510650708894)
		(1000000000, 0.009352677348322527)
		
		};
		
		\addplot coordinates {
		(100, 0.11268818784362933)
		(1000, 0.09235632391954453)
		(10000, 0.06085183414294938)
		(100000, 0.8792864002944591)
		(1000000, 0.2516692022670629)
		(10000000, 0.05720230171308279)
		(100000000, 0.09686973651528466)
		(1000000000, 0.015503076214066998)
		
		};
		
		\addplot coordinates {
		(100, 4.367921607554625)
		(1000, 2.5867560883837033)
		(10000, 0.19267169396738515)
		(100000, 0.040107614912353436)
		(1000000, 0.4730480931810351)
		(10000000, 0.12037961490266519)
		(100000000, 0.039864793792599225)
		(1000000000, 0.007970675963974338)
		
		};
		
		\addplot coordinates {
		(100, 2.652328485461903)
		(1000, 2.492576004717528)
		(10000, 1.6978626520383377)
		(100000, 1.147083576080773)
		(1000000, 0.7564426865409155)
		(10000000, 0.3796532603114533)
		(100000000, 0.05581613530071868)
		(1000000000, 0.017534814849470455)
		
		};
		\end{semilogxaxis}
		\end{tikzpicture}
		\begin{tikzpicture}
		\begin{semilogxaxis}[
		scale only axis,
		width=0.4\textwidth,
		log basis x=10,
		log basis y=10,
		ylabel near ticks,
		height=4cm,
		ymax=7,
		ymin=-0.7,
		xlabel=$n$, 
		ylabel=Rel. Deviation,
		title={Optimised Program},
		grid=major,
		legend entries={$p=1$,$p=2$,$p=4$,$p=8$,$p=16$},
		legend pos=north east]
		\addplot coordinates {
		(100, 0.8687065697065419)
		(1000, 0.0799914979806646)
		(10000, 0.02067251364240473)
		(100000, 0.009121704515100443)
		(1000000, 0.013494599693025836)
		(10000000, 0.08534650801677054)
		(100000000, 0.08168939962827118)
		(1000000000, 0.014598962720540154)
		
		};
		
		\addplot coordinates {
		(100, 6.574204637264616)
		(1000, 0.04145585748301211)
		(10000, 0.029523532536101297)
		(100000, 0.806830570679923)
		(1000000, 0.029201241156600208)
		(10000000, 0.0847646784780893)
		(100000000, 0.055673461478022525)
		(1000000000, 0.023508879272757122)
		
		};
		
		\addplot coordinates {
		(100, 0.10807480178922649)
		(1000, 0.08850313225937673)
		(10000, 0.4214093855883432)
		(100000, 1.061044119979381)
		(1000000, 0.1741535452998721)
		(10000000, 0.12907815614428186)
		(100000000, 0.1255578413818158)
		(1000000000, 0.008339998856569752)
		
		};
		
		\addplot coordinates {
		(100, 2.675417731532113)
		(1000, 2.023031176887656)
		(10000, 2.3191086389231472)
		(100000, 0.16406770559612774)
		(1000000, 0.305689773744763)
		(10000000, 0.1291424157428458)
		(100000000, 0.0997100269349392)
		(1000000000, 0.005162268199345419)
		
		};
		
		\addplot coordinates {
		(100, 2.5428150071275124)
		(1000, 3.611545332160197)
		(10000, 1.776395545957089)
		(100000, 1.8983872605331447)
		(1000000, 1.0257164507406025)
		(10000000, 0.4474433103897337)
		(100000000, 0.1606753105874427)
		(1000000000, 0.0311778237194774)
		
		};
		\end{semilogxaxis}
		\end{tikzpicture}
\end{figure}	
\end{minipage}

Now, let us analyse how the program performs with up to 16 processors.
This benchmark has been made both for the optimised and for the unoptimised program.
The running times of both programs can be seen in~\autoref{fig:par_opt_sieve}.
Each value in this figure is the mean running time of 100~iterations to minimise short-term disturbances.
This has been made for $p\in\{1,2,4,8,16\}$ and $n\in\{10^i\mid i\in\{2,\dots,9\} \}$.
To make sure that these mean values are representative, the relative standard deviation can be anlaysed in~\autoref{fig:par_sieve_rel_deviation} as well.
As expected, for larger values of $n$ the relative standard deviation of the running time get smaller for any number of processes.

Looking at~\autoref{fig:par_opt_sieve} we notice, that the parallelisation only has a positive effect once $n$ reaches at least $10^6$.
The reason that below $10^6$ more processor means longer running time is due to the communication of the coordinator.
This can also be seen by looking at the coefficient of $g$ in the BSP cost:
Only for high $n$ the benefit of making the computation time smaller makes up the increase in communication time.
For $n=10^9$ the speed-up with $16$ processors then is roughly $9.4$.
Interestingly using $8$ processors has already a speed-up of roughly $7.0$ for $n=10^9$.
A problem that this speedup did not show up for $p=16$ as high may have been, that there were quite some other processes running in the background using up some of the capacities of the 16 processors.


\section{Extensions and Applications}

\subsection{Twin Primes}

A twin prime is a pair of primes that differ only by two.
Examples of such twin primes are $4\pm1, 6\pm1, 12\pm1$.
It is not yet known, whether there are infinitely many twin primes, although some efforts have already been made.
For example, Yitang Zhuang showed in 2014, that there are infinitely many pairs $(p_i, p_{i+1})$ of primes with $p_{i+1} - p_i < 7\cdot 10^7$.
We want compute twin primes in a parallel algorithm extending the parallel Sieve of Eratosthenes.

The resulting program can be seen in \autoref{prg:twin-primes}. It has as input a distributed list of primes according to the block distribution and returns all twin primes up to $n$.

This program works as follows:
As we know, that the  primes lists in each processor are sorted,
we can just iterate through these lists and check if consecutive primes have distance $2$.
This works for all twin primes except these that live in two different processors.
A workaround for finding also these divided twins is, to make every processor tell the consecutive processor the largest prime of himself.
Then the processor can first check whether this previous prime is twin with the first prime of the processor.

We analyse the BSP cost of this subprogram:
We assume that assignments can be ignored.
Then for the first superstep there is only one comparison which counts as BSP cost.
The next superstep is a simple 1-relation with a cost of $6+g$.
The last superstep involves the actual computation.
With the approximation $n/\ln n$ for the number of primes smaller or equal to $n$, we may assume, that the number of primes in the first block are maximal.
Hence we can bound the number of computations in the last superstep by approximately $6\cdot (n/p) / \ln(n/p)$.
Summing the BSP cost can be estimated by
\[
	7 + 6\cdot\frac{n}{p\ln(n/p)} + g + 3l.
\]

Running this algorithm with $n=200$ generates the following twin primes:
\begin{align*}
4\pm1,
6\pm1,
12\pm1,
18\pm1, 
30\pm1, 
42\pm1, 
60\pm1, 
72\pm1,\\
102\pm1,
108\pm1,
138\pm1,
150\pm1,
180\pm1,
192\pm1,
198\pm1.
\end{align*}
%A fundamental theorem with regards to twin primes is Brun's Theorem:
%\begin{theorem}
%	\cite[Theorem~6.33]{LeVeque2014}
%	The series of reciprocals of twin primes either is a finite sum or forms a convergent series:
%	\[
%	\left(\frac{1}{3} + \frac{1}{5}\right) + \left(\frac{1}{5} + \frac{1}{7}\right) + \cdots < \infty.
%	\]
%	In fact, if $\pi_2(x)$ is the number of such pairs not exceeding $x$, then
%	$\pi_2(x) = \bigO(\frac{x(\log\log x)^2}{\log^2 x})$.
%\end{theorem}


\subsection{Goldbach Conjecture}

We want to discuss another number theoretic question, that the mathematician Christian Goldbach asked in the 18th century:
Is every even number $k > 2$ the sum of two prime numbers?
The so-called Goldbach conjecture suggests, that the answer to this question is yes.
We want to check this conjecture up until a large number $k$ by the means of a parallel program.

We again extend the functionality of the parallel Sieve of Eratosthenes.
Similar to the actual sieve, we use a boolean array distributed blockwise which should have value $\mathbf{true}$ for an index $k$, if the algorithm has found two primes summing to $k$ and therefore crossed out $k$.

The problem here is, that some processor $s$ has to know all primes of the previous processors in order to check any possible combination of two prime numbers.
This exchange is made possible by first exchanging the number of primes in the individual blocks and then reserving a large enough array to hold all of the necessary primes.

Defining an array length $L\coloneqq \ceil{n/p}$ a processor $s\in\{0, \dots, p\}$ takes care of all even numbers in $[s\cdot L, (s+1)\cdot L)$.
As the sums of primes of processors $i$ and $j$ are contained within $[(i+j)\cdot L, (i+j+2)\cdot L)$, we only need to check combinations of blocks from processors $i$ and $j$ if $i+j=s$ or $i+j+1=s$.

\newcommand{\low}{\texttt{low}}\newcommand{\high}{\texttt{high}}
One possibility to systematically obtain all these combinations is to first initialise two variables $\low$ and $\high$ with the value $\floor{s / 2}$.
We will now gradually increase $\high$ up to $s$ and decrease $\low$ while making sure not to skip any combination.
After processing the pair $(\low, \high)$ we update these variable in the following manner:
If $\low + \high + 1 = s$, we will increment $\high$ such that after this update we have $\low + \high = s$.
If on the other hand $\low + \high = s$ is satisfied, we will decrement $\low$, such that thereafter $\low + \high + 1 = s$ holds.
Doing this until either $\low$ is smaller than $0$ or $\high$ is greater than $s$, we processed every such pair.

\autoref{prg:twin-primes} implements the above procedure.
It processes one such pair $(\low, \high)$ by first transferring another processor's block of primes to the local memory, if it has not been loaded yet.
Again, this is done by checking if $\low + \high + 1 = s$: If this is the case, we have just decremented $\low$ and hence we want to load the primes of processor $\low$.
Otherwise we want to load the ones of processor $\high$ analogously, except if $\high = s$ holds.

Let us also discuss the BSP cost of this extension program:
The first superstep where only variables are initialized is negligible.
In the second superstep, the exchange of the numbers of the primes introduces a cost of $p\cdot g$.
In the next superstep, the array for the exchange of primes is prepared, where all primes of the current processor are copied into.
Similar to the last section, we may assume that the first processor has the most primes, namely approximately $(n/p)/\ln(n/p)$.
Also the \texttt{crosses} array is initialised which needs $n/(2p)$ operations.
Hence the cost of this superstep is bounded by $(n/p) / \ln(n/p) + n/(2p)$ computation operations.

Now, we get to the main loop, which has exactly $p$ iterations.
This is, because processor $p-1$ has the most combinations of blocks of primes to check and for $p-1$ it takes exactly $p$ steps to get from $\low=\high=\floor{(p-1)/2}$ to either $\low = -1$ or $\high = s+1$, because in each step alternately either $\high$ is increased or $\low$ is decreased by one.

In the first superstep of the loop there is a single $\texttt{bsp}\_\texttt{get}$ for getting the requested block of primes,
which introduces a cost of $(n/p)/\ln(n/p) \cdot g$.
The subsequent superstep is the most expensive one:
Here we go trough (almost) all possibilities taking one prime from the block of $\low$ and one from the block of $\high$ and crossing out the number if it falls into the correct block.
This has at worst a cost of \[
\left(\frac{n/p}{\ln(n/p)}\right)^2 = \frac{n^2}{p^2 \ln(n/p)^2}.\]
For checking if the conjecture holds, we need another $n/p$ steps.
Hence the overall accumulated BSP cost is bounded by
\[
	\frac{n}{p \ln(n/p)} + \frac{n}{2p} + \frac{n^2}{p \ln(n/p)^2} + \left( p + \frac{n}{\ln(n/p)} \right)\cdot g + (3+p)\cdot l.
\]



Using this program, the Goldbach conjecture was successfully checked for all even numbers smaller or equal to $10.000.000$.

\pagebreak
\appendix
\section{Appendix}

 	\begin{lstlisting}[caption={Sequential implementation of the Sieve of Eratosthenes}, label={prg:seq_sieve}, frame={single}]
#include <cstdio>
#include <cmath>
#include <vector>

long numberOps = 0;

std::vector<long> sieve(long n) {
  bool *crosses = new bool[n - 1]();
  // list[i] == true means i+2 is crossed out.
  // new bool[n-1]() automatically initialises the array with false.
  double maxIter = sqrt(n) - 2;
  numberOps += n + 1;
  for (long i = 0; i <= maxIter; i++) {
    if (!crosses[i]) { // cross out all larger multiples of i+2
      for (int j = (i+2)*(i+2) - 2; j < n - 1; j += (i+2)) {
        crosses[j] = true;
        numberOps += 3;
      }
    }
    numberOps += 3;
  }

  std::vector<long> primes;
  primes.reserve(n / log(n)); // n/ln(n) is average number of primes <= n
  for (long i = 0; i < n - 1; i++) {
    if (!crosses[i]) primes.push_back(i+2);
    numberOps += 4;
  }

  delete [] crosses;
  return primes;
}
	\end{lstlisting}

	\begin{lstlisting}[caption={Optimised parallel program for the Sieve of Eratosthenes}, label={prg:par_opt_sieve}, frame={single}]
#include <bsp.hpp>
#include <cmath>
#include <vector>

long P, n; // P: number of processors

void bsp_sieve_optimised() {
  bsp_begin(P);
  double time0 = bsp_time();
  long p = bsp_nprocs(), s = bsp_pid();

  // We only consider odd numbers to save on memory and enhance performance
  // We include 1 for convenience
  long globalArrayLength = n / 2;
  long arrayLength = ceil(((double) globalArrayLength) / p);
  long blockStart = 2 * s * arrayLength + 1;
  bool *crosses = new bool[arrayLength]();
  if (s == 0) crosses[0] = true; // Cross out 1
  // We start sieving with prime 3; the first coordinator is processor 0.
  long currPrime = 3, currCoordinator = 0, lastCoordinator = 0;
  bsp_push_reg(&currPrime, sizeof(long));
  bsp_push_reg(&currCoordinator, sizeof(long));
  bsp_sync();

  long maxSievePrime = (long) sqrt(n);
  while (currPrime <= maxSievePrime) {
    // Sieve, if the coordinator did not change
    if (lastCoordinator == currCoordinator)
      sieve_optimised(currPrime, crosses, blockStart, arrayLength);

    // Let coordinator change current prime or coordinator
    if (s == currCoordinator) {
      // Search for the next prime in local array
      long j = (currPrime - blockStart) / 2
               + (lastCoordinator == currCoordinator);
      while (j < arrayLength && crosses[j]) j++;
      // Translate back to normal representation
      currPrime = blockStart + j * 2;
      lastCoordinator = currCoordinator;
      // If j exceeds the local array, transfer coordination to next proc.
      currCoordinator = j < arrayLength ? s : s + 1;
    } else lastCoordinator = currCoordinator;
    bsp_sync();

    // Distribute information
    if (s == lastCoordinator) {
      for (long k = 0; k < p; k++) {
        bsp_put(k, &currCoordinator, &currCoordinator, 0, sizeof(long));
        bsp_put(k, &currPrime, &currPrime, 0, sizeof(long));
      }
    }
    bsp_sync();
  }

  // Accumulate all local primes in list
  std::vector<long> primes;
  primes.reserve(n / log(n)); // n/ln(n) is average number of primes <= n
  if (s == 0) primes.push_back(2);
  for (long j = 0; j < arrayLength; j++) {
    if (!crosses[j] && blockStart + 2 * j <= n)
      primes.push_back(blockStart + 2 * j);
  }
  delete[] crosses;
  bsp_pop_reg(&currPrime);
  bsp_pop_reg(&currCoordinator);
  bsp_sync();

  double time1 = bsp_time();
  if (s == 0) {
    timeTaken = time1 - time0;
  }
  
  // Do something with primes

  primes.clear();
  bsp_end();
}

void sieve_optimised(long prime, bool * crosses,
                     long blockStart, long arrayLength) {
  // Search for first multiple of prime in local array
  // Start with prime^2 and check if it is prior to the local array
  long j = (prime*prime - blockStart) / 2;
  if (j < 0) {
    // If so, take the first multiple of prime in local array:
    long mod = blockStart % prime;
    // If the remainder `mod` is ...
    if (mod == 0) j = 0;// zero, j=0 represents the first multiple
    else if (mod%2 == 1) j = (prime - mod)/2; // odd, advance by prime-mod.
    else j = (2 * prime - mod) / 2; // even, advance by 2*prime-mod
  }
  // Now cross out all multiples of the prime starting with j
  while (j < arrayLength) {
    crosses[j] = true;
    j += prime; // Step size: 2*prime/2; ignore even multiples.
  }
}
	\end{lstlisting}

	\begin{lstlisting}[caption={Extension function for finding twin primes}, label={prg:twin-primes}, frame={single}]
#include <bsp.hpp>
#include <vector>

/* Returns list of twins represented by their mean */
std::vector<long> bsp_twins(std::vector<long> primes) {
  long p = bsp_nprocs(), s = bsp_pid();
  // Get largest prime of previous processor
  long prevPrime = s == 0 ? 2 : 0;
  bsp_push_reg(&prevPrime, sizeof(long));
  bsp_sync();

  if (s < p - 1 && primes.size() > 0)
    bsp_put(s + 1, &primes[primes.size() - 1],
            &prevPrime, 0, sizeof(long));
  bsp_sync();

  std::vector<long> twins;

  if (primes.size() > 0 && prevPrime + 2 == primes[0])
    twins.push_back(prevPrime + 1);
  for (long i = 0; i < primes.size() - 1; i++) {
    if (primes[i] + 2 == primes[i + 1])
      twins.push_back(primes[i] + 1);
  }

  bsp_pop_reg(&prevPrime);
  bsp_sync();
  return twins;
}
	\end{lstlisting}
	
\pagebreak
	\begin{lstlisting}[caption={Extension function for checking the Goldbach conjecture}, label={prg:goldbach}, frame={single}]
#include <bsp.hpp>
#include <cstdio>
#include <cstring>
#include <vector>

void bsp_goldbach(std::vector<long> primes, long n) {
  long p = bsp_nprocs(), s = bsp_pid();
  long globalArrayLength = ceil(((double) n) / 2);
  long arrayLength = ceil(((double) globalArrayLength) / p);
  // proc i manages [2*i*arrayLength, ..., 2*(i+1)*arrayLength)

  long * numberPrimes = new long[p];
  bsp_push_reg(numberPrimes, p*sizeof(long));
  numberPrimes[s] = primes.size();
  bsp_sync();

  for (long i = s + 1; i < p; i++)
    bsp_put(i, &numberPrimes[s], numberPrimes,
            s*sizeof(long), sizeof(long));
  bsp_sync();

  // Make space for necessary primes
  long numberNecPrimes = 0;
  for (long i = 0; i <= s; i++) numberNecPrimes += numberPrimes[i];
  long * necPrimes = new long [numberNecPrimes];
  long ** primeBlocks = new long*[s+1]; // Indirection for convenience
  primeBlocks[0] = necPrimes;
  for (long i = 1; i <= s; i++)
    primeBlocks[i] = primeBlocks[i-1] + numberPrimes[i-1];
  std::copy(primes.begin(), primes.end(), primeBlocks[s]);
  bsp_push_reg(necPrimes, numberNecPrimes*sizeof(long));

  bool * crosses = new bool[arrayLength]();
  // Cross out 0 and 2.
  if (s == 0) crosses[0] = crosses[1] = true;
  bsp_sync();

  // Every processor needs to cross out
  // using blocks i,j with i+j=s or i+j+1=s.
  // There are exactly p iterations.
  long high, low;
  high = low = s/2;
  for (long iter = 0; iter < p; iter++) {
    if (low < 0 || high > s) {
      bsp_sync();
      continue;
    }
    if (low + high == s && high != s)
      bsp_get(high, necPrimes, (primeBlocks[high]-necPrimes)*sizeof(long),
              primeBlocks[high], numberPrimes[high]*sizeof(long));
    else if (low + high + 1 == s)
      bsp_get(low, necPrimes, (primeBlocks[low]-necPrimes)*sizeof(long),
              primeBlocks[low], numberPrimes[low]*sizeof(long));
    bsp_sync();

    for (long i = 0; i < numberPrimes[high]; i++) {
      long maxPrime = (s+1)*2*arrayLength - primeBlocks[high][i];
      for (long j = 0;
           j < numberPrimes[low] && primeBlocks[low][j] < maxPrime; j++) {
        long crossOut = primeBlocks[high][i] + primeBlocks[low][j];
        if (crossOut%2 == 0 && crossOut >= s*2*arrayLength)
          crosses[(crossOut - s*2*arrayLength) / 2] = true;
      }
    }
    if (low + high + 1 == s) high++;
    else low--;
  }

  // Check the conjecture
  for (long i = 0; i < arrayLength; i++) {
    if (!crosses[i] && (s*arrayLength + i)*2 <= n)
      printf("Found counterexample: %li!\n", (s*arrayLength + i)*2);
  }

  delete [] crosses;
  bsp_pop_reg(necPrimes);
  bsp_pop_reg(numberPrimes);
  delete [] primeBlocks;
  delete [] numberPrimes;
}
	\end{lstlisting}

\end{document}